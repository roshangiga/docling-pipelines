{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docling Pipelines: All use-cases + LLM Enrichment + Toggles\n",
    "\n",
    "This notebook lets you:\n",
    "- Run all scripts in `scripts/` (with short descriptions).\n",
    "- Toggle Formula Understanding and Picture Description before conversion.\n",
    "- Select LLM provider via env (gemini or gpt) and enrich chunks with structured outputs.\n",
    "\n",
    "Inputs go in `source/`. Outputs are written to `output/`."
   ],
   "id": "4fd2a6694ba3bbf6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install dependencies (safe to re-run)\n",
    "!pip install -q -r requirements.txt"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "708c4e28803ad107"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load environment and configure provider\nimport os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n# Load .env if present. You can copy env.example to .env and fill keys.\nif Path('.env').exists():\n    load_dotenv('.env')\nelse:\n    load_dotenv()  # load from environment only\n\n# Provider: 'gemini' or 'gpt' (from env PROVIDER)\nPROVIDER = os.getenv('PROVIDER', 'gemini').strip().lower()\nOPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')\nGEMINI_MODEL = os.getenv('GEMINI_MODEL', 'gemini-2.5-flash')\n\nprint('Provider:', PROVIDER)\nprint('OPENAI_MODEL:', OPENAI_MODEL)\nprint('GEMINI_MODEL:', GEMINI_MODEL)",
   "execution_count": null,
   "outputs": [],
   "id": "71b4dd6db1a8fa4f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Paths\n",
    "project_root = Path().resolve()\n",
    "src_dir = project_root / 'source'\n",
    "out_dir = project_root / 'output'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "print('Project root:', project_root)\n",
    "print('Source dir:', src_dir)\n",
    "print('Output dir:', out_dir)\n",
    "print('Source files:', list(src_dir.glob('*')))"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "e947cec027435afe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Scripts quick-run (what each does)"
   ],
   "id": "d0e8e3106e50fddb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `general_convert.py`: basic PDF/URL conversion to Markdown/JSON."
   ],
   "id": "8f7567c6144ebee8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%run scripts/general_convert.py"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "5f888e70adfa86c7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vlm_image_understanding.py`: VLM (SmolDocling) for image-heavy PDFs."
   ],
   "id": "e1e5b20abe5ce1cc"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%run scripts/vlm_image_understanding.py"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "4ca8a767ab6374d9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `maths_processing.py`: converts and extracts math snippets heuristically."
   ],
   "id": "2f672e6feba7e6b2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%run scripts/maths_processing.py"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "775c92c1957a2e56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `contextual_hybrid_chunking.py`: HybridChunker raw + contextualized chunks."
   ],
   "id": "c318110daad0130e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%run scripts/contextual_hybrid_chunking.py"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "3a15d6360df3b370"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `enrich_formula_understanding.py`: Formula Understanding enrichment (LaTeX/MathML)."
   ],
   "id": "3ec35f935b691a68"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%run scripts/enrich_formula_understanding.py"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "85c63806ca2256c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `enrich_picture_description.py`: Picture Description enrichment (VLM captions)."
   ],
   "id": "e8c1a4954cac5703"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%run scripts/enrich_picture_description.py"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "77bc7114f3e66652"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) One-pass combination: picture description + formula enrichment + contextual chunkings + LLM enrichment options\n",
    "- Toggle Formula/Picture in the cell below.\n",
    "- Select provider via PROVIDER in .env (gemini/gpt).\n",
    "- Produces TXT/JSONL of enriched chunks (structured)."
   ],
   "id": "7424f9f38dadf87f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Toggles (set True/False or override via env if you prefer)\nDO_FORMULA = True   # <- set False to disable Formula Understanding\nDO_PICTURE = True   # <- set False to disable Picture Description\nDO_LLM = True       # <- set False to disable LLM enrichment\n\nprint('DO_FORMULA:', DO_FORMULA, '| DO_PICTURE:', DO_PICTURE, '| DO_LLM:', DO_LLM)\n\nfrom datetime import datetime\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.chunking import HybridChunker\n\n# Configure Docling pipeline based on toggles\npdf_opts = PdfPipelineOptions()\npdf_opts.do_formula_enrichment = bool(DO_FORMULA)\npdf_opts.do_picture_description = bool(DO_PICTURE)\n# Tip: you could choose a picture description preset here if desired\n# from docling.datamodel.pipeline_options import smolvlm_picture_description\n# pdf_opts.picture_description_options = smolvlm_picture_description\n\nconverter = DocumentConverter(\n    format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_opts)}\n)\n\n# Pick a PDF\npdf_path = next((p for p in src_dir.glob('*.pdf')), None)\nassert pdf_path is not None, 'Put a PDF into source/'\n\ndl_doc = converter.convert(str(pdf_path)).document\n\n# Chunk\nchunker = HybridChunker()\nchunks = list(chunker.chunk(dl_doc=dl_doc))\n\n# LLM enrichment (conditional on DO_LLM)\nif DO_LLM:\n    # Create selected LLM client\n    if PROVIDER == 'gpt':\n        from clients.openai_client import OpenAIClient, EnrichedChunk\n        llm = OpenAIClient(model=OPENAI_MODEL)\n    elif PROVIDER == 'gemini':\n        from clients.gemini_client import GeminiClient, EnrichedChunk\n        llm = GeminiClient(model=GEMINI_MODEL)\n    else:\n        raise ValueError(f'Unsupported PROVIDER: {PROVIDER}')\n\n# Write outputs\nts = datetime.now().strftime('%Y%m%d_%H%M%S')\nbase = f'{pdf_path.stem}__combo_llm__{PROVIDER}__{ts}' if DO_LLM else f'{pdf_path.stem}__combo_basic__{ts}'\ntxt_path = out_dir / f'{base}.txt'\njsonl_path = out_dir / f'{base}.jsonl'\n\nimport json\nwith txt_path.open('w', encoding='utf-8') as f_txt, jsonl_path.open('w', encoding='utf-8') as f_jsonl:\n    for i, ch in enumerate(chunks):\n        raw = ch.text or ''\n        structural = chunker.contextualize(chunk=ch)\n        \n        if DO_LLM:\n            enriched = llm.enrich_chunk(raw, context=structural)  # Pydantic validated\n            # TXT with LLM enrichment\n            f_txt.write(f'=== {i} ===\\n')\n            f_txt.write('-- title --\\n' + (enriched.title or '') + '\\n')\n            f_txt.write('-- summary --\\n' + enriched.summary + '\\n')\n            f_txt.write('-- key_points --\\n' + '\\n'.join('- ' + kp for kp in enriched.key_points) + '\\n')\n            f_txt.write('-- enriched_text --\\n' + enriched.enriched_text + '\\n\\n')\n            # JSONL with LLM enrichment\n            f_jsonl.write(json.dumps({\n                'index': i,\n                'title': enriched.title,\n                'summary': enriched.summary,\n                'key_points': enriched.key_points,\n                'enriched_text': enriched.enriched_text,\n                'path': getattr(ch, 'path', None),\n                'id': getattr(ch, 'id', None),\n            }, ensure_ascii=False) + '\\n')\n        else:\n            # Basic output without LLM enrichment\n            f_txt.write(f'=== {i} ===\\n')\n            f_txt.write('-- raw_text --\\n' + raw + '\\n')\n            f_txt.write('-- structural_context --\\n' + structural + '\\n\\n')\n            # JSONL without LLM enrichment\n            f_jsonl.write(json.dumps({\n                'index': i,\n                'raw_text': raw,\n                'structural_context': structural,\n                'path': getattr(ch, 'path', None),\n                'id': getattr(ch, 'id', None),\n            }, ensure_ascii=False) + '\\n')\n\nprint('Wrote:', txt_path)\nprint('Wrote:', jsonl_path)",
   "execution_count": null,
   "outputs": [],
   "id": "bd36358565daed80"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Inspect outputs"
   ],
   "id": "58979a6e4442da8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for p in sorted(out_dir.glob('*')):\n",
    "    print(p.name)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "379750bdf53fb38b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
